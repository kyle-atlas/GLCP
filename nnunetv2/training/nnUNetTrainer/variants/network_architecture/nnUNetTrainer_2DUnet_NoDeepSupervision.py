import os
import torch
from torch import nn
from torch.nn.parallel import DistributedDataParallel as DDP

from nnunetv2.training.nnUNetTrainer.variants.network_architecture.nnUNetTrainerNoDeepSupervision import nnUNetTrainerNoDeepSupervision
from nnunetv2.utilities.plans_handling.plans_handler import ConfigurationManager, PlansManager
from nnunetv2.utilities.label_handling.label_handling import determine_num_input_channels
# from nnunetv2.training.nnUNetTrainer.variants.network_architecture.Unet3D import Unet3D
# from nnunetv2.training.nnUNetTrainer.variants.network_architecture.Unet2D_nnunet import Unet2D
from nnunetv2.training.nnUNetTrainer.variants.network_architecture.Unet2D_nnunet_fullres import Unet2D



class nnUNetTrainer_2DUnet_NoDeepSupervision(nnUNetTrainerNoDeepSupervision):
    def initialize(self):
        if not self.was_initialized:
            self.num_input_channels = determine_num_input_channels(self.plans_manager, self.configuration_manager,
                                                                   self.dataset_json)
            
            label_manager = self.plans_manager.get_label_manager(self.dataset_json)

            # print("label_manager---", label_manager)


            patch_size = self.configuration_manager.patch_size
            self.num_input_channels, label_manager.num_segmentation_heads
            
            patch_size_tuple = tuple(patch_size)
            # self.network = SwinUNETR(img_size=patch_size_tuple, in_channels=self.num_input_channels, out_channels=label_manager.num_segmentation_heads, spatial_dims=len(patch_size), use_v2=False).to(self.device)
            self.network = Unet2D(in_channels=3, out_channels=label_manager.num_segmentation_heads).to(self.device)
            print( self.network )
            # compile network for free speedup
            if ('nnUNet_compile' in os.environ.keys()) and (
                    os.environ['nnUNet_compile'].lower() in ('true', '1', 't')):
                self.print_to_log_file('Compiling network...')
                self.network = torch.compile(self.network)

            self.optimizer, self.lr_scheduler = self.configure_optimizers()
            # if ddp, wrap in DDP wrapper
            if self.is_ddp:
                self.network = torch.nn.SyncBatchNorm.convert_sync_batchnorm(self.network)
                self.network = DDP(self.network, device_ids=[self.local_rank])

            self.loss = self._build_loss()
            self.was_initialized = True
        else:
            raise RuntimeError("You have called self.initialize even though the trainer was already initialized. "
                               "That should not happen.")

    @staticmethod
    def build_network_architecture(plans_manager: PlansManager,
                                   dataset_json,
                                   configuration_manager: ConfigurationManager,
                                   num_input_channels,
                                   enable_deep_supervision: bool = True) -> nn.Module:

        num_input_channels = determine_num_input_channels(plans_manager, configuration_manager,
                                                                   dataset_json)
        
        label_manager = plans_manager.get_label_manager(dataset_json)

        patch_size = configuration_manager.patch_size
        patch_size_tuple = tuple(patch_size)
        # network = SwinUNETR(img_size=patch_size_tuple, in_channels=num_input_channels, out_channels=label_manager.num_segmentation_heads, spatial_dims=len(patch_size), use_v2=False)
        network = Unet2D(in_channels=3, out_channels=label_manager.num_segmentation_heads)

        return network